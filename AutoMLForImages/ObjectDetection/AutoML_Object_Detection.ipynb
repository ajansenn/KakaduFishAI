{"cells":[{"attachments":{},"cell_type":"markdown","id":"d6706d7d-d544-416c-ba80-5879d9b96050","metadata":{"tags":[]},"source":["Copyright (c) Microsoft Corporation. All rights reserved.\n","\n","Licensed under the MIT License.\n","# Kakadu Fish AI\n","## Training an Object Detection model using AutoML\n","\n","This notebook was adapted from [Microsofts Azure Machine Learning examples](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/tutorials/automl-with-azureml) \n","\n","In this notebook, we use AutoML for training an Object Detection model. We will use a custom dataset in COCO format to train the model, tune hyperparameters of the model to optimize model performance and deploy the model to use in inference scenarios."]},{"cell_type":"markdown","id":"45579aca-996f-4523-aed5-57a914770890","metadata":{},"source":["### Workspace setup\n","\n","In order to train and deploy models in Azure ML, you will first need to set up a workspace.\n","\n","An Azure ML Workspace is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML Workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, deployment, inference, and the monitoring of deployed models.\n","\n","Create an Azure ML Workspace within your Azure subscription, or load an existing workspace.\n"]},{"cell_type":"code","execution_count":null,"id":"318152db-6e4d-4fb4-8b03-d0f6f936ec0e","metadata":{},"outputs":[],"source":["## specify workspace parameters, these can be obtained from your azure subscription and AML account\n","\n","subscription_id='<insert subscription id>'   \n","resource_group='<insert resource group name>'   \n","workspace_name='<insert AML workspace name>'\n","\n","from azureml.core.workspace import Workspace\n","ws = Workspace.from_config()"]},{"cell_type":"markdown","id":"104a8772-e677-48e4-abee-5529ba5ca1fe","metadata":{},"source":["### Compute target setup\n","\n","You will need to provide a Compute Target that will be used for your AutoML model training. AutoML models for image tasks require GPU SKUs and support NC and ND families. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model.\n"]},{"cell_type":"code","execution_count":null,"id":"89f6e58e-a59c-4d11-9b7d-0943350bb87a","metadata":{},"outputs":[],"source":["from azureml.core.compute import AmlCompute, ComputeTarget\n","\n","cluster_name = \"gpu-cluster-nc6\"\n","\n","try:\n","    compute_target = ws.compute_targets[cluster_name]\n","    print('Found existing compute target.')\n","except KeyError:\n","    print('Creating a new compute target...')\n","    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6', \n","                                                           idle_seconds_before_scaledown=1800,\n","                                                           min_nodes=0, \n","                                                           max_nodes=4)\n","\n","    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n","    \n","# Can poll for a minimum number of nodes and for a specific timeout.\n","# If no min_node_count is provided, it will use the scale settings for the cluster.\n","compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"]},{"cell_type":"markdown","id":"be491a6b-0d20-424d-817f-698670a78ed6","metadata":{},"source":["### Experiment Setup\n","Create an Experiment in your workspace to track your model training runs. Give your experiment a name. This name will appear in the experiments section in the AML Studio. "]},{"cell_type":"code","execution_count":null,"id":"ffc27c0d-bd88-45ed-b853-702b0d9d6fce","metadata":{},"outputs":[],"source":["from azureml.core import Experiment\n","\n","experiment_name = '<create name for your experiment>' \n","experiment = Experiment(ws, name=experiment_name)"]},{"cell_type":"markdown","id":"e111fb1e-06ec-4fd9-86fd-18ab97cdabef","metadata":{},"source":["### Dataset with input Training Data\n","In order to generate models for computer vision, you will need to bring in labeled image data as input for model training in the form of an AzureML Labeled Dataset. You can either use a Labeled Dataset that you have exported from a Data Labeling project, or create a new Labeled Dataset with your labeled training data\n","\n"]},{"cell_type":"code","execution_count":null,"id":"39b5e0dc","metadata":{},"outputs":[],"source":["from IPython.display import Image\n","Image(filename='000000001020.jpg') "]},{"cell_type":"markdown","id":"edb482ad-6a3e-4a18-ae95-d46607052a95","metadata":{},"source":["#### Convert annotation file from COCO to JSONL\n","If you want to try with a dataset in COCO format, the scripts below shows how to convert it to jsonl format. "]},{"cell_type":"code","execution_count":null,"id":"e23099f0-f324-4615-867f-e2153decc1f6","metadata":{},"outputs":[],"source":["#provide the credentials for the storage account where your training dataset and COCO .json file reside.\n","\n","from azure.storage.blob import BlobServiceClient\n","import pandas as pd\n","import json\n","\n","STORAGEACCOUNTURL= 'https://<insert account name here>.blob.core.windows.net'\n","STORAGEACCOUNTKEY= '<insert storage account key here>'\n","CONTAINERNAME= '<specify blob container name>'\n","BLOBNAME= '<insert .json file name here>.json'"]},{"cell_type":"code","execution_count":null,"id":"5791f4ec-9147-4962-8b5a-7445b82be614","metadata":{},"outputs":[],"source":["#change the name below to your .json file name (withopen(\"enter name here\",\"w\")\n","\n","blob_service_client_instance = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\n","blob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\n","with open(BLOBNAME, \"wb\") as my_blob:\n","    blob_data = blob_client_instance.download_blob()\n","    coco_data = json.loads(blob_data.readall())\n","\n","with open(\"<insert json file name here>.json\", \"w\") as outfile:\n","    outfile.write(json.dumps(coco_data))"]},{"cell_type":"code","execution_count":null,"id":"3ffe25ec-8afd-4e03-a643-709c4953a2d7","metadata":{},"outputs":[],"source":["# Generate jsonl file from coco file. Change the input_coco_file_path string to the name of your COCO .json file, the output name to the new name of your .jsonl file, and the base_url to the name of the datastore where your images and COCO .json file are.\n","\n","!python coco2jsonl.py --input_coco_file_path \"./<insert json file name here>.json\" --output_dir \".\" --output_file_name \"<insert new file name here>.jsonl\" --task_type \"ObjectDetection\" --base_url \"AmlDatastore://<insert datastore name here>/\""]},{"cell_type":"markdown","id":"b5c4fcd1-9fa6-4407-9b34-84ed670bbe3e","metadata":{},"source":["### Upload the JSONL file and images to Datastore\n","In order to use the data for training in Azure ML, we upload it to our Azure ML Workspace via a Datastore. The datastore provides a mechanism for you to upload/download data, and interact with it from your remote compute targets. It is an abstraction over Azure Storage."]},{"cell_type":"code","execution_count":null,"id":"8e5164a0-1d7f-41e7-89ef-bb4c65e8dca0","metadata":{},"outputs":[],"source":["# Retrieving default datastore that got automatically created when we setup a workspace. Ensure the datastore name and .jsonl name (used above) are correct below.\n","\n","from azureml.core import Workspace, Datastore\n","ds = Datastore.get(ws, '<insert datastore name here>')\n","print(ds.name)\n","ds.upload_files(files = ['<insert jsonl file name created above here>.jsonl'], overwrite=True)"]},{"cell_type":"markdown","id":"9adf139a-51fc-4525-ac81-c277f2d2d30c","metadata":{},"source":["Finally, we need to create an Azure ML Dataset from the data we uploaded to the Datastore. We create one dataset for training and one for validation."]},{"cell_type":"code","execution_count":null,"id":"3a5aab3d-96ef-4971-a406-d3f0f0f9e181","metadata":{},"outputs":[],"source":["# give a name to the dataset that'll be created from your converted .jsonl file and ensure the name of the .jsonl file specified below is correct.\n","\n","from azureml.contrib.dataset.labeled_dataset import _LabeledDatasetFactory, LabeledDatasetTask\n","from azureml.core import Dataset\n","\n","training_dataset_name = '<create dataset name here>'\n","if training_dataset_name in ws.datasets:\n","    training_dataset = ws.datasets.get(training_dataset_name)\n","    print('Found the training dataset', training_dataset_name)\n","else:\n","    # create training dataset\n","    training_dataset = _LabeledDatasetFactory.from_json_lines(\n","        task=LabeledDatasetTask.OBJECT_DETECTION, path=ds.path('<insert jsonl name here>.jsonl'))\n","    training_dataset = training_dataset.register(workspace=ws, name=training_dataset_name)\n","    \n","    \n","print(\"Training dataset name: \" + training_dataset.name)"]},{"cell_type":"markdown","id":"257911d1-fa04-42ca-87ab-b64a2c03f21b","metadata":{},"source":["Validation dataset is optional. If no validation dataset is specified, by default 20% of your training data will be used for validation. You can control the percentage using the split_ratio argument - please refer to the documentation for more details.\n","\n","This is what the training dataset looks like:"]},{"cell_type":"code","execution_count":null,"id":"b6b45900-5def-432c-96be-3f3afbca3899","metadata":{},"outputs":[],"source":["# check the dataframe looks correct\n","\n","training_dataset.to_pandas_dataframe()"]},{"cell_type":"markdown","id":"0af4190d-6a41-4d3f-9cc0-d009530b8518","metadata":{"tags":[]},"source":["### Configuring your AutoML run for image tasks\n","AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm to be used, specify hyperparameter values for your model as well as perform a sweep across the hyperparameter space to generate an optimal model. Parameters for configuring your AutoML runs for image related tasks are specified using the AutoMLImageConfig - please refer to the documentation for the details on the parameters that can be used and their values.\n","\n"]},{"cell_type":"markdown","id":"428ebeb3-89e4-4812-bb9f-34520da8cba7","metadata":{},"source":["When using AutoML for image tasks, you need to specify the model algorithms using the model_name parameter. You can either specify a single model or choose to sweep over multiple ones. Currently supported model algorithms for object detection: yolov5, fasterrcnn_resnet50_fpn, fasterrcnn_resnet34_fpn, fasterrcnn_resnet18_fpn, retinanet_resnet50_fpn."]},{"cell_type":"markdown","id":"ca6b64ea-d1a5-4f9e-9b2e-e7a99173d73d","metadata":{},"source":["#### Using default hyperparameter values for the specified algorithm\n","Before doing a large sweep to search for the optimal models and hyperparameters, we recommend trying the default values to get a first baseline. Next, you can explore multiple hyperparameters for the same model before sweeping over multiple models and their parameters. This is for employing a more iterative approach, because with multiple models and multiple hyperparameters for each (as we showcase in the next section), the search space grows exponentially and you need more iterations to find optimal configurations.\n","\n","If you wish to use the default hyperparameter values for a given algorithm (say yolov5), you can specify the config for your AutoML Image runs as follows:"]},{"cell_type":"code","execution_count":null,"id":"485b5759-661b-4625-95a3-2323d39e0f6b","metadata":{"tags":[]},"outputs":[],"source":["\n","from azureml.train.automl import AutoMLImageConfig\n","from azureml.train.hyperdrive import GridParameterSampling, choice\n","\n","image_config_yolov5 = AutoMLImageConfig(task='image-object-detection',\n","                                        compute_target=compute_target,\n","                                        training_data=training_dataset,\n","                                        hyperparameter_sampling=GridParameterSampling({'model_name': choice('yolov5')}))"]},{"cell_type":"markdown","id":"4a29699e-2976-46e3-be19-c878158e295f","metadata":{},"source":["#### Submitting an AutoML run for Image tasks\n","Once you've created the config settings for your run, you can submit an AutoML run using the config in order to train an image model using your training dataset."]},{"attachments":{},"cell_type":"markdown","id":"6cc1b08e-5dd8-4ec4-941c-78ac6fceacf9","metadata":{},"source":["#### Add tags to the experiment\n","\n","Add tags to the experiment so the metadata can be used to parse training runs. At a minimum enter the tags listed below to the experiment.submit(automl_image_config, tags={})\n"]},{"cell_type":"code","execution_count":null,"id":"1c93a265-c2b4-4ba6-aee8-4c3be4c118ea","metadata":{},"outputs":[],"source":["# this submits the experiment to the GPU-cluster specified earlier. You can track the experiment progress in the experiment tab from Azure Studio.\n","\n","automl_image_run = experiment.submit(image_config_yolov5)"]},{"cell_type":"markdown","id":"dd6ca7ba-0ee6-4901-9126-58b78c471c88","metadata":{},"source":["### Hyperparameter sweeping for your AutoML models for image tasks\n","In this example, we use the AutoMLImageConfig to train an Object Detection model using yolov5 and fasterrcnn_resnet50_fpn, both of which are pretrained on COCO, a large-scale object detection, segmentation, and captioning dataset that contains over 200K labeled images with over 80 label cateogories.\n","\n","When using AutoML for image tasks, you can perform a hyperparameter sweep over a defined parameter space, to find the optimal model. In this example, we sweep over the hyperparameters for each algorithm, choosing from a range of values for learning_rate, optimizer, lr_scheduler, etc, to generate a model with the optimal primary metric. If hyperparameter values are not specified, then default values are used for the specified algorithm.\n","\n","We use Random Sampling to pick samples from this parameter space and try a total of 20 iterations with these different samples, running 4 iterations at a time on our compute target, which has been previously set up using 4 nodes. Please note that the more parameters the space has, the more iterations you need to find optimal models.\n","\n","We also leverage the Bandit early termination policy that terminates poor performing configs (those that are not within 20% slack of the best perfroming config), thus significantly saving compute resources.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ecb97ba6-28a2-4cf3-8cf6-a5fe77c4ffce","metadata":{},"outputs":[],"source":["from azureml.train.automl import AutoMLImageConfig\n","from azureml.train.hyperdrive import GridParameterSampling, RandomParameterSampling, BayesianParameterSampling\n","from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n","from azureml.train.hyperdrive import choice, uniform\n","\n","parameter_space = {\n","    'model': choice(\n","        {\n","            'model_name': choice('yolov5'),\n","            'learning_rate': uniform(0.0001, 0.01),\n","            #'model_size': choice('small', 'medium'), # model-specific\n","            'img_size': choice(640, 704, 768), # model-specific\n","        },\n","        {\n","            'model_name': choice('fasterrcnn_resnet50_fpn'),\n","            'learning_rate': uniform(0.0001, 0.001),\n","            #'warmup_cosine_lr_warmup_epochs': choice(0, 3),\n","            'optimizer': choice('sgd', 'adam', 'adamw'),\n","            'min_size': choice(600, 800), # model-specific\n","        }\n","    )\n","}\n","\n","tuning_settings = {\n","    'iterations': 20, \n","    'max_concurrent_iterations': 4, \n","    'hyperparameter_sampling': RandomParameterSampling(parameter_space),  \n","    'policy': BanditPolicy(evaluation_interval=2, slack_factor=0.2, delay_evaluation=6)\n","}\n","\n","\n","automl_image_config = AutoMLImageConfig(task='image-object-detection',\n","                                        compute_target=compute_target,\n","                                        training_data=training_dataset,\n","                                        primary_metric='mean_average_precision',\n","                                        **tuning_settings)"]},{"attachments":{},"cell_type":"markdown","id":"d5a98e7c-7ef8-41a4-bdee-58cce7042501","metadata":{},"source":["#### Add tags to the experiment\n","\n","Add tags to the experiment so the metadata can be used to parse training runs. At a minimum enter the tags listed below to the experiment.submit(automl_image_config, tags={})\n"]},{"cell_type":"code","execution_count":null,"id":"5d6b0fb3-3000-4d70-af48-90ac1abc9477","metadata":{},"outputs":[],"source":["# this submits the experiment to the GPU-cluster specified earlier. You can track the experiment progress in the experiment tab from Azure Studio.\n","\n","automl_image_run = experiment.submit(automl_image_config)"]},{"cell_type":"markdown","id":"c5713e19-167c-4456-b8f6-657d8e1f6634","metadata":{},"source":["When doing a hyperparameter sweep, it can be useful to visualize the different configurations that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child runs' tab in the UI of the main automl_image_run from above, which is the HyperDrive parent run. Then you can go into the 'Child runs' tab of this one. Alternatively, here below you can see directly the HyperDrive parent run and navigate to its 'Child runs' tab:"]},{"cell_type":"code","execution_count":null,"id":"f5691f15-96d6-4369-bd96-ba48ce7bdc13","metadata":{},"outputs":[],"source":["# running this cell will produce the \n","\n","from azureml.core import Run\n","hyperdrive_run = Run(experiment=experiment, run_id=automl_image_run.id + '_HD')\n","hyperdrive_run"]},{"cell_type":"markdown","id":"7c262ab0-13b0-4136-9131-0b38092e8755","metadata":{},"source":["## Register the optimal model from the AutoML run\n","Once the run completes, we can register the model that was created from the best run (configuration that resulted in the best primary metric)"]},{"cell_type":"code","execution_count":null,"id":"f388a228-2219-42c2-a088-c4e11dbf34fc","metadata":{},"outputs":[],"source":["# Register the model from the best run\n","\n","best_child_run = automl_image_run.get_best_child()\n","model_name = best_child_run.properties['model_name']\n","model = best_child_run.register_model(model_name = model_name, model_path='outputs/model.pt')"]},{"cell_type":"markdown","id":"e830d2c6-ddf5-4bc1-9c20-ed84585b1d9a","metadata":{},"source":["#### Deploy model as a web service\n","Once you have your trained model, you can deploy the model on Azure. You can deploy your trained model as a web service on Azure Container Instances (ACI) or Azure Kubernetes Service (AKS). ACI is the perfect option for testing deployments, while AKS is better suited for for high-scale, production usage.\n","In this tutorial, we will deploy the model as a web service in AKS."]},{"cell_type":"markdown","id":"f342232b-134b-43b9-8a4d-677a8200737d","metadata":{},"source":["You will need to first create an AKS compute cluster, or use an existing AKS cluster. You can use either GPU or CPU VM SKUs for your deployment cluster"]},{"cell_type":"code","execution_count":null,"id":"cff68a18-a2bb-4198-885f-2b11194af90a","metadata":{},"outputs":[],"source":["from azureml.core.compute import ComputeTarget, AksCompute\n","from azureml.exceptions import ComputeTargetException\n","\n","# Choose a name for your cluster\n","aks_name = \"cluster-aks-gpu\"\n","\n","# Check to see if the cluster already exists\n","try:\n","    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n","    print('Found existing compute target')\n","except ComputeTargetException:\n","    print('Creating a new compute target...')\n","    # Provision AKS cluster with GPU machine\n","    prov_config = AksCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\", \n","                                                        location=\"australiaeast\")\n","    # Create the cluster\n","    aks_target = ComputeTarget.create(workspace=ws, \n","                                      name=aks_name, \n","                                      provisioning_configuration=prov_config)\n","    aks_target.wait_for_completion(show_output=True)"]},{"cell_type":"markdown","id":"155f0891-71b6-4a24-a0db-7cef6f4d61c0","metadata":{},"source":["Next, you will need to define the inference configuration, that describes how to set up the web-service containing your model. You can use the scoring script and the environment from the training run in your inference config.\n","\n","Note: To change the model's settings, open the downloaded scoring script and modify the model_settings variable before deploying the model."]},{"cell_type":"code","execution_count":null,"id":"ca96a4ae-fe8c-495f-971d-aee39910e44d","metadata":{},"outputs":[],"source":["from azureml.core.model import InferenceConfig\n","\n","best_child_run.download_file('outputs/scoring_file_v_1_0_0.py', output_file_path='score.py')\n","environment = best_child_run.get_environment()\n","inference_config = InferenceConfig(entry_script='score.py', environment=environment)"]},{"cell_type":"markdown","id":"41355026-7ffb-4b83-9391-1387a9b88e38","metadata":{},"source":["You can then deploy the model as an AKS web service."]},{"cell_type":"code","execution_count":null,"id":"eb419f7a-5951-4787-bac0-672b6017b7d9","metadata":{},"outputs":[],"source":["# Deploy the model from the best run as an AKS web service\n","# remeber to give your aka service a name below \n","\n","from azureml.core.webservice import AksWebservice\n","from azureml.core.webservice import Webservice\n","from azureml.core.model import Model\n","from azureml.core.environment import Environment\n","\n","aks_config = AksWebservice.deploy_configuration(autoscale_enabled=True,                                                    \n","                                                cpu_cores=1,\n","                                                memory_gb=50,\n","                                                enable_app_insights=True)\n","\n","aks_service = Model.deploy(ws,\n","                           models=[model],\n","                           inference_config=inference_config,\n","                           deployment_config=aks_config,\n","                           deployment_target=aks_target,\n","                           name='kakadufish-aks-endpoint',\n","                           overwrite=True)\n","aks_service.wait_for_deployment(show_output=True)\n","print(aks_service.state)"]},{"cell_type":"markdown","id":"d4626e43-e654-4547-83ea-eba916f9e594","metadata":{},"source":["## Test the web service\n","Finally, let's test our deployed web service to predict new images. You can pass in any image. In this case, we'll use a random image from the dataset and pass it to te scoring URI."]},{"cell_type":"code","execution_count":null,"id":"79ed2c17-54a3-4043-8166-a4867be9c3e1","metadata":{},"outputs":[],"source":["import requests\n","\n","# URL for the web service\n","scoring_uri = aks_service.scoring_uri\n","\n","# If the service is authenticated, set the key or token\n","key, _ = aks_service.get_keys()\n","\n","sample_image = './000000001020.jpg'\n","\n","# Load image data\n","data = open(sample_image, 'rb').read()\n","\n","# Set the content type\n","headers = {'Content-Type': 'application/octet-stream'}\n","\n","# If authentication is enabled, set the authorization header\n","headers['Authorization'] = f'Bearer {key}'\n","\n","# Make the request and display the response\n","resp = requests.post(scoring_uri, data, headers=headers)\n","print(resp.text)"]},{"cell_type":"markdown","id":"7f5797d7-52af-481f-bccd-5024744249bd","metadata":{},"source":["#### Visualize detections\n","Now that we have scored a test image, we can visualize the bounding boxes for this image"]},{"cell_type":"code","execution_count":null,"id":"079fe2a6-9413-4452-80ca-2301067c8dd0","metadata":{},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import matplotlib.patches as patches\n","from PIL import Image\n","import numpy as np\n","import json\n","\n","IMAGE_SIZE = (18,12)\n","plt.figure(figsize=IMAGE_SIZE)\n","img_np=mpimg.imread(sample_image)\n","img = Image.fromarray(img_np.astype('uint8'),'RGB')\n","x, y = img.size\n","\n","fig,ax = plt.subplots(1, figsize=(15,15))\n","# Display the image\n","ax.imshow(img_np)\n","\n","# draw box and label for each detection \n","detections = json.loads(resp.text)\n","for detect in detections['boxes']:\n","    label = detect['label']\n","    box = detect['box']\n","    conf_score = detect['score']\n","    if conf_score > 0.6:\n","        ymin, xmin, ymax, xmax =  box['topY'],box['topX'], box['bottomY'],box['bottomX']\n","        topleft_x, topleft_y = x * xmin, y * ymin\n","        width, height = x * (xmax - xmin), y * (ymax - ymin)\n","        print('{}: [{}, {}, {}, {}], {}'.format(detect['label'], round(topleft_x, 3), \n","                                                round(topleft_y, 3), round(width, 3), \n","                                                round(height, 3), round(conf_score, 3)))\n","\n","        color = np.random.rand(3) #'red'\n","        rect = patches.Rectangle((topleft_x, topleft_y), width, height, \n","                                 linewidth=3, edgecolor=color,facecolor='none')\n","\n","        ax.add_patch(rect)\n","        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n","\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5bbd0e74114ad0b277c5b9fb4504daaeac032fe9115eb2a1dcc303b3dc066136"}}},"nbformat":4,"nbformat_minor":5}
